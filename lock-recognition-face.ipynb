{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ComputerVision #Pre-TrainedModel \n",
    "#TransferLearningModels #VGG-16 #DataAugmentation\n",
    "\n",
    "#filters #padding #pooling #flattening.\n",
    "\n",
    "https://towardsdatascience.com/smart-face-lock-system-6c5a77aa5d30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Collect the images of the legal owner for which the face recognition model will grant permission..\n",
    "2. Create an additional folder if we want to add more people who can have access to our system.\n",
    "3. Resize the images into a (224, 224, 3) shape so that we can pass it through the VGG-16 architecture. \n",
    "    \n",
    "    Note that VGG-16 architecture is pre-trained on the image net weights which have the aforementioned shape.\n",
    "\n",
    "4. Create variations of the images by performing the image data augmentation on the datasets.\n",
    "5. Create our custom model on top of the VGG-16 architecture by excluding the top layer.\n",
    "6. Compilation, training, and fitting the models accordingly with the essential callbacks.\n",
    "7. Conclude with a final model that can load the weights of the model and perform the face recognition based smart face lock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the opencv module for computer vision and capturing images.\n",
    "import cv2\n",
    "#Importing os module to access the local system.\n",
    "import os\n",
    "#We are going to “on” our default webcam and then proceed to capture the images of our faces which is required for the dataset.\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "directory = \"images/folder_train/\"\n",
    "path = os.listdir(directory)\n",
    "\n",
    "#to label our images from 0 to the total number of photos we click.\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key%256 == 32:\n",
    "        img_path = directory + str(count) + \".jpeg\"\n",
    "        cv2.imwrite(img_path, frame)\n",
    "        count += 1\n",
    "    \n",
    "    elif key%256 == 113: \n",
    "        break\n",
    "\n",
    "#After we exit the program we will release the video capture from our webcam and destroy the cv2 graphical window.\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. *Resizing the Images:* Reshape our images we collected into a size that is suitable to pass through the VGG-16 architecture which is pre-trained of the imagenet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"folder1_train\"\n",
    "path = os.listdir(directory)\n",
    "\n",
    "for i in path:\n",
    "    img_path = directory + i\n",
    "    image = cv2.imread(img_path)\n",
    "    #We are rescaling all our images captured from the default frame size to (224, 224) pixels because that is what’s best if we want to try out transfer learning models like VGG-16.\n",
    "    #We have already captured the images in an RGB format. Thus we already have 3 channels and we do not need to specify that. The required number of channels for a VGG-16 is 3 \n",
    "    #and the architecture is ideally of shape (224, 224, 3).\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    \n",
    "    cv2.imwrite(img_path, image)#After the resizing step is completed, we can transfer the Owner’s directory into the images folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. *Image Data Augmentation:* The ImageDataGenerator is used for data augmentation of images.\n",
    "We will be replicating and making copies of the transformations of the\n",
    "original images. The Keras Data Generator will use the copies and\n",
    "not the original ones. This will be useful for training at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#import tensorflow as tf\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "directory = \"images\"\n",
    "#parametros que seran usados para el DataAugmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=30,\n",
    "                                   shear_range=0.3,\n",
    "                                   zoom_range=0.3,\n",
    "                                   width_shift_range=0.4,\n",
    "                                   height_shift_range=0.4,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "#aplicacion de los parametros de DataAugmentation a las imagenes deseadas\n",
    "Img_Height=224\n",
    "Img_width=224\n",
    "batch_size=128\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory,\n",
    "                                                    target_size=(Img_Height, Img_width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras proporciona una interfaz de aplicaciones para cargar y usar modelos previamente entrenados.\n",
    "\n",
    "\n",
    "Con esta interfaz, **puede crear un modelo VGG utilizando los pesos previamente entrenados proporcionados por el grupo Oxford y usarlo como punto de partida en su propio modelo**, o usarlo como modelo directamente para clasificar imágenes.\n",
    "\n",
    "Keras provides both the 16-layer and 19-layer version via the VGG16 and VGG19 classes. Let’s focus on the VGG16 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Face Recognition Model we are building will be trained by using transfer learning. We will be using the VGG-16 model with # no top layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, we are importing the VGG-16 Model in the variable VGG16_MODEL and making sure we input the model without the top layer.\n",
    "\n",
    "#Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import vgg16\n",
    "\n",
    "VGG16_MODEL = vgg16.VGG16(input_shape=(Img_width, Img_Height, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "for layers in VGG16_MODEL.layers: \n",
    "    layers.trainable=False\n",
    "\n",
    "#for layers in VGG16_MODEL.layers:\n",
    "#    print(layers.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SNOWFALL](https://miro.medium.com/max/1400/0*usI_HmpFeF2iPBEM.png)\n",
    "\n",
    "Al agregar múltiples capas convolucionales y capas de agrupación, la imagen se procesará para la extracción de características. Y habrá capas completamente conectadas que se dirijan a la capa para la función softmax (para un caso de varias clases) o sigmoide (para un caso binario).\n",
    "\n",
    "Adding multiple convolutional layers and pooling layers, the image will be processed for feature extraction. And there will be fully connected layers heading to the layer for softmax (for a multi-class case) or sigmoid (for a binary case) function\n",
    "\n",
    "Understanding CNN is understanding how the image data is processed. How to transfer images into data and extract the features for prediction. From this basic structure, there are many modified and converged versions. ResNet, AlexNet, VGG-16 or Inception Networks are some of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Custom layer consists of the input layer which is, basically the output of the VGG-16 Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense # dense = tf.keras.layers.Dense\n",
    "\n",
    "#-----------------------------------------------------------------PROCESSING DATA LAYERS--------------------------------------------------------------------------------------------\n",
    "\n",
    "#   I   N   P   U   T       M   O   D   E   L       V   G   G   1   6       L   A   Y   E   R\n",
    "\n",
    "# tengo el modelo entrenado to flama sin la ultima capa. la ultima capa la tengo que personalizar para mis fines.\n",
    "input_layer = VGG16_MODEL.output\n",
    "\n",
    "\n",
    "#   C   U   S   T   O   M       L   A   Y   E   R\n",
    "\n",
    "# Convolutional Layer\n",
    "# We add a convolutional layer with 32 filters, kernel_size of (3,3), and default strides of (1,1) and we use activation as relu with he_normal as the initializer.\n",
    "Conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='valid',\n",
    "               data_format='channels_last', activation='relu', \n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=0), \n",
    "               name='Conv1')(input_layer)\n",
    "\n",
    "# MaxPool Layer\n",
    "# Usaremos la (capa de agrupación/pool layer) para reducir la muestra de las capas de la capa convolucional.\n",
    "# La agrupación máxima es un tipo de operación que generalmente se agrega a las CNN después de capas convolucionales individuales.\n",
    "# Cuando se agrega a un modelo, la agrupación máxima reduce la dimensionalidad de las imágenes al reducir la cantidad de píxeles en la salida de la capa convolucional anterior.\n",
    "# Podría verse como una pérdida de información a primera vista, pero es más bien obtener datos más \"significativos\" que perderlos.\n",
    "# Al eliminar algo de ruido en los datos y extraer solo el significativo, podemos reducir el sobreajuste/overfitting y acelerar el cálculo.\n",
    "Pool1 = tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2),padding='valid', \n",
    "                  data_format='channels_last',name='Pool1')(Conv1)\n",
    "\n",
    "#-----------------------------------------------------------------FLATTEN LAYER------------------------------------------------------------------------------------------------\n",
    "# Flatten\n",
    "# Estamos creando un modelo de clasificación, lo que significa que estos datos procesados ​​deberían ser una buena entrada para el modelo. Debe tener la forma de un vector lineal unidimensional. \n",
    "# Las formas rectangulares o cúbicas no pueden ser entradas directas.\n",
    "# Y está conectado al modelo de clasificación final, que se denomina capa totalmente conectada.\n",
    "flatten = tf.keras.layers.Flatten(data_format='channels_last',name='Flatten')(Pool1)\n",
    "\n",
    "#---------------------------------------------------------CLASSIFICATION MODEL  /   FULLY CONNECTED LAYER-----------------------------------------------------------------------\n",
    "# Fully Connected layer-1\n",
    "# Las 2 capas completamente conectadas se utilizan con activación como relu, es decir, un Arquitectura densa después de pasar la muestra por una capa aplanadora/flatten layer.\n",
    "FC1 = tf.keras.layers.Dense(units=30, activation='relu', \n",
    "            kernel_initializer=tf.keras.initializers.glorot_normal(seed=32), \n",
    "            name='FC1')(flatten)\n",
    "\n",
    "# Fully Connected layer-2\n",
    "FC2 = Dense(units=30, activation='relu', \n",
    "            kernel_initializer=tf.keras.initializers.glorot_normal(seed=33),\n",
    "            name='FC2')(FC1)\n",
    "\n",
    "#-------------------------------------------------------------OUTPUT LAYER-----------------------------------------------------------------------------------\n",
    "# Output layer\n",
    "# La capa de salida tiene una activación softmax con num_classes es 2 que predice las probabilidades de num_classes, es decir, el propietario autorizado o un participante adicional o una cara rechazada.\n",
    "num_classes=2\n",
    "Out = Dense(units=num_classes, activation='softmax', \n",
    "            kernel_initializer=tf.keras.initializers.glorot_normal(seed=3), \n",
    "            name='Output')(FC2)\n",
    "\n",
    "model1 = tf.keras.Model(inputs=VGG16_MODEL.input,outputs=Out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. And it is connected to the final classification model, which is called a fully-connected layer. In other words, we put all the pixel data in one line and make connections with the final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SNOWFALL](https://miro.medium.com/max/1400/1*IWUxuBpqn2VuV-7Ubr01ng.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CallBacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be importing the 3 required callbacks for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# ModelCheckPoint \n",
    "# This callback is used for storing the weights of our model after training. \n",
    "# We save only the best weights of our model by specifying save_best_only=True. We will monitor our training by using the accuracy metric.\n",
    "checkpoint = ModelCheckpoint(\"face_rec.h5\", monitor='accuracy', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# This callback is used for reducing the learning rate of the optimizer after a specified number of epochs. Here, we have specified the patience as 10. \n",
    "# If the accuracy does not improve after 10 epochs, then our learning rate is reduced accordingly by a factor of 0.2. The metric used for monitoring here is accuracy as well.\n",
    "reduce = ReduceLROnPlateau(monitor='accuracy', factor=0.2, patience=3, min_lr=0.00001, verbose = 1)\n",
    "\n",
    "# Tensorboard\n",
    "# Is used for plotting the visualization of the graphs, namely the graph plots for accuracy and the loss.\n",
    "logdir='logsface'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir, histogram_freq=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0939 - accuracy: 1.0000\n",
      "Epoch 1: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 8s 8s/step - loss: 3.0939 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.9239 - accuracy: 1.0000\n",
      "Epoch 2: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.9239 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0905 - accuracy: 1.0000\n",
      "Epoch 3: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3.0905 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.8400 - accuracy: 1.0000\n",
      "Epoch 4: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2.8400 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.2335 - accuracy: 1.0000\n",
      "Epoch 5: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3.2335 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.1205 - accuracy: 1.0000\n",
      "Epoch 6: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3.1205 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.9582 - accuracy: 1.0000\n",
      "Epoch 7: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2.9582 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.9279 - accuracy: 1.0000\n",
      "Epoch 8: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2.9279 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0400 - accuracy: 1.0000\n",
      "Epoch 9: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3.0400 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0597 - accuracy: 1.0000\n",
      "Epoch 10: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.0597 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.8906 - accuracy: 1.0000\n",
      "Epoch 11: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.8906 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0312 - accuracy: 1.0000\n",
      "Epoch 12: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3.0312 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0237 - accuracy: 1.0000\n",
      "Epoch 13: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.0237 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0983 - accuracy: 1.0000\n",
      "Epoch 14: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3.0983 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0596 - accuracy: 1.0000\n",
      "Epoch 15: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.0596 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.2711 - accuracy: 1.0000\n",
      "Epoch 16: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.2711 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0762 - accuracy: 1.0000\n",
      "Epoch 17: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.0762 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.2119 - accuracy: 1.0000\n",
      "Epoch 18: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.2119 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.1261 - accuracy: 1.0000\n",
      "Epoch 19: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.1261 - accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.9597 - accuracy: 1.0000\n",
      "Epoch 20: accuracy did not improve from 1.00000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.9597 - accuracy: 1.0000 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20f3b8dd210>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "epochs = 20\n",
    "\n",
    "model1.fit(train_generator,\n",
    "           epochs = epochs,\n",
    "           callbacks = [checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mm\\Documents\\ComputerScience\\IA\\ml-projects\\lock-recognition-face\\lock-recognition-face.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/IA/ml-projects/lock-recognition-face/lock-recognition-face.ipynb#ch0000024?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/IA/ml-projects/lock-recognition-face/lock-recognition-face.ipynb#ch0000024?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvis_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_model\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mm/Documents/ComputerScience/IA/ml-projects/lock-recognition-face/lock-recognition-face.ipynb#ch0000024?line=3'>4</a>\u001b[0m keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mplot_model(model1, to_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel1.png\u001b[39m\u001b[39m'\u001b[39m, show_layer_names\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model1, to_file='model1.png', show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "300dbfc6b0ad0831e999f42922719e8c3270874e67c762ad6f8e521060e0a9f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
